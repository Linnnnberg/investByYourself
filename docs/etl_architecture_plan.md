# ETL & Database Architecture Plan - investByYourself

## üìã **Story-005: ETL & Database Architecture Design**

**Status**: üöß IN PROGRESS
**Priority**: Critical
**Effort**: Very High
**Timeline**: Weeks 4-6
**Dependencies**: Story-001, Tech-006

---

## üèóÔ∏è **Architecture Overview**

### **High-Level Design**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Data Sources  ‚îÇ    ‚îÇ  ETL Pipeline   ‚îÇ    ‚îÇ   Database      ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Yahoo Finance ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ ‚Ä¢ Extract       ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ ‚Ä¢ PostgreSQL   ‚îÇ
‚îÇ ‚Ä¢ Alpha Vantage ‚îÇ    ‚îÇ ‚Ä¢ Transform     ‚îÇ    ‚îÇ ‚Ä¢ Redis Cache  ‚îÇ
‚îÇ ‚Ä¢ FRED API      ‚îÇ    ‚îÇ ‚Ä¢ Load          ‚îÇ    ‚îÇ ‚Ä¢ Data Lake    ‚îÇ
‚îÇ ‚Ä¢ API Ninjas    ‚îÇ    ‚îÇ ‚Ä¢ Validate      ‚îÇ    ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Core Components**
1. **Data Collection Layer** - External API integrations
2. **Data Processing Engine** - Transformation and validation
3. **Data Storage Layer** - Database and caching systems
4. **Data Quality Layer** - Validation and monitoring
5. **Orchestration Layer** - Scheduling and workflow management

---

## üîÑ **ETL Process Flow**

### **1. Data Extraction (E)**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    EXTRACTION LAYER                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Rate Limiting & Throttling                               ‚îÇ
‚îÇ ‚Ä¢ Error Handling & Retry Logic                             ‚îÇ
‚îÇ ‚Ä¢ Data Source Health Monitoring                            ‚îÇ
‚îÇ ‚Ä¢ Incremental Data Collection                              ‚îÇ
‚îÇ ‚Ä¢ API Response Validation                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **2. Data Transformation (T)**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  TRANSFORMATION LAYER                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Data Standardization & Normalization                     ‚îÇ
‚îÇ ‚Ä¢ Business Logic Application                               ‚îÇ
‚îÇ ‚Ä¢ Data Quality Checks & Cleaning                           ‚îÇ
‚îÇ ‚Ä¢ Aggregation & Calculation                                ‚îÇ
‚îÇ ‚Ä¢ Schema Mapping & Conversion                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **3. Data Loading (L)**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     LOADING LAYER                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Incremental Updates                                      ‚îÇ
‚îÇ ‚Ä¢ Data Versioning & History                                ‚îÇ
‚îÇ ‚Ä¢ Conflict Resolution                                      ‚îÇ
‚îÇ ‚Ä¢ Performance Optimization                                 ‚îÇ
‚îÇ ‚Ä¢ Backup & Recovery                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä **Data Sources & Integration**

### **Primary Data Sources**

#### **1. Yahoo Finance (yfinance)**
- **Data Types**: Stock prices, fundamentals, options, dividends
- **Update Frequency**: Real-time (prices), Daily (fundamentals)
- **Rate Limits**: 2000 requests/hour
- **Integration**: Python yfinance library

#### **2. Alpha Vantage**
- **Data Types**: Technical indicators, fundamental data, forex
- **Update Frequency**: Real-time, Daily
- **Rate Limits**: 5 requests/minute (free), 500/minute (paid)
- **Integration**: REST API with Python requests

#### **3. FRED (Federal Reserve)**
- **Data Types**: Economic indicators, interest rates, inflation
- **Update Frequency**: Monthly, Quarterly
- **Rate Limits**: 120 requests/minute
- **Integration**: Python fredapi library

#### **4. API Ninjas (Earnings & Transcripts)**
- **Data Types**: Earnings data, call transcripts
- **Update Frequency**: Daily (earnings), Real-time (transcripts)
- **Rate Limits**: 50,000 requests/month
- **Integration**: REST API

### **Data Source Priority Matrix**
| Source | Priority | Data Quality | Cost | Reliability | Integration Effort |
|--------|----------|--------------|------|-------------|-------------------|
| Yahoo Finance | High | High | Free | High | Low |
| Alpha Vantage | Medium | High | Low | High | Medium |
| FRED | High | Very High | Free | Very High | Low |
| API Ninjas | Medium | Medium | Low | Medium | Medium |

---

## üóÑÔ∏è **Database Architecture**

### **Database Technology Stack**
- **Primary Database**: PostgreSQL 15+ (ACID compliance, JSON support)
- **Caching Layer**: Redis 7+ (in-memory, persistence)
- **Data Lake**: MinIO (S3-compatible object storage)
- **Search Engine**: Elasticsearch 8+ (full-text search)

### **Database Schema Design**

#### **Core Financial Entities**

##### **1. Companies Table**
```sql
CREATE TABLE companies (
    id SERIAL PRIMARY KEY,
    symbol VARCHAR(10) UNIQUE NOT NULL,
    name VARCHAR(255) NOT NULL,
    sector VARCHAR(100),
    industry VARCHAR(100),
    market_cap DECIMAL(20,2),
    pe_ratio DECIMAL(10,4),
    dividend_yield DECIMAL(5,4),
    beta DECIMAL(6,4),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    data_source VARCHAR(50),
    is_active BOOLEAN DEFAULT TRUE
);

CREATE INDEX idx_companies_symbol ON companies(symbol);
CREATE INDEX idx_companies_sector ON companies(sector);
CREATE INDEX idx_companies_market_cap ON companies(market_cap);
```

##### **2. Stock Prices Table**
```sql
CREATE TABLE stock_prices (
    id SERIAL PRIMARY KEY,
    company_id INTEGER REFERENCES companies(id),
    date DATE NOT NULL,
    open_price DECIMAL(10,4),
    high_price DECIMAL(10,4),
    low_price DECIMAL(10,4),
    close_price DECIMAL(10,4),
    adjusted_close DECIMAL(10,4),
    volume BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    UNIQUE(company_id, date)
);

CREATE INDEX idx_stock_prices_company_date ON stock_prices(company_id, date);
CREATE INDEX idx_stock_prices_date ON stock_prices(date);
```

##### **3. Economic Indicators Table**
```sql
CREATE TABLE economic_indicators (
    id SERIAL PRIMARY KEY,
    indicator_code VARCHAR(20) UNIQUE NOT NULL,
    indicator_name VARCHAR(255) NOT NULL,
    category VARCHAR(100),
    frequency VARCHAR(20),
    units VARCHAR(50),
    source VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE economic_data (
    id SERIAL PRIMARY KEY,
    indicator_id INTEGER REFERENCES economic_indicators(id),
    date DATE NOT NULL,
    value DECIMAL(15,6),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    UNIQUE(indicator_id, date)
);
```

##### **4. Portfolios Table**
```sql
CREATE TABLE portfolios (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE portfolio_holdings (
    id SERIAL PRIMARY KEY,
    portfolio_id INTEGER REFERENCES portfolios(id),
    company_id INTEGER REFERENCES companies(id),
    shares DECIMAL(15,6) NOT NULL,
    cost_basis DECIMAL(10,4),
    purchase_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

##### **5. Earnings Data Table**
```sql
CREATE TABLE earnings (
    id SERIAL PRIMARY KEY,
    company_id INTEGER REFERENCES companies(id),
    quarter VARCHAR(7) NOT NULL, -- Format: YYYY-Q1
    report_date DATE,
    eps_estimate DECIMAL(8,4),
    eps_actual DECIMAL(8,4),
    revenue_estimate DECIMAL(15,2),
    revenue_actual DECIMAL(15,2),
    surprise_percentage DECIMAL(8,4),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    UNIQUE(company_id, quarter)
);

CREATE TABLE earnings_call_transcripts (
    id SERIAL PRIMARY KEY,
    earnings_id INTEGER REFERENCES earnings(id),
    transcript_text TEXT,
    sentiment_score DECIMAL(3,2), -- -1.0 to 1.0
    key_metrics JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### **Database Performance Optimizations**

#### **1. Partitioning Strategy**
```sql
-- Partition stock_prices by year for better query performance
CREATE TABLE stock_prices_2024 PARTITION OF stock_prices
FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');

CREATE TABLE stock_prices_2025 PARTITION OF stock_prices
FOR VALUES FROM ('2025-01-01') TO ('2026-01-01');
```

#### **2. Materialized Views**
```sql
-- Daily portfolio performance summary
CREATE MATERIALIZED VIEW portfolio_daily_summary AS
SELECT
    p.id as portfolio_id,
    p.name as portfolio_name,
    ph.company_id,
    c.symbol,
    ph.shares,
    sp.close_price,
    (ph.shares * sp.close_price) as current_value,
    (ph.shares * sp.close_price - ph.shares * ph.cost_basis) as pnl,
    sp.date
FROM portfolios p
JOIN portfolio_holdings ph ON p.id = ph.portfolio_id
JOIN companies c ON ph.company_id = c.id
JOIN stock_prices sp ON c.id = sp.company_id
WHERE sp.date >= CURRENT_DATE - INTERVAL '30 days';

CREATE UNIQUE INDEX idx_portfolio_daily_summary
ON portfolio_daily_summary(portfolio_id, company_id, date);
```

---

## ‚öôÔ∏è **Technical Implementation**

### **Core ETL Classes**

#### **1. Data Collector Base Class**
```python
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
import asyncio
import aiohttp
import time
from datetime import datetime, timedelta

class DataCollector(ABC):
    """Abstract base class for data collectors."""

    def __init__(self, api_key: str, rate_limit: int = 100):
        self.api_key = api_key
        self.rate_limit = rate_limit
        self.last_request_time = 0
        self.request_count = 0
        self.session: Optional[aiohttp.ClientSession] = None

    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def _rate_limit_check(self):
        """Implement rate limiting logic."""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time

        if time_since_last < (1.0 / self.rate_limit):
            await asyncio.sleep((1.0 / self.rate_limit) - time_since_last)

        self.last_request_time = time.time()
        self.request_count += 1

    @abstractmethod
    async def collect_data(self, **kwargs) -> Dict[str, Any]:
        """Collect data from the source."""
        pass

    @abstractmethod
    async def validate_response(self, response: Dict[str, Any]) -> bool:
        """Validate API response."""
        pass
```

#### **2. Yahoo Finance Collector**
```python
import yfinance as yf
from typing import Dict, Any, List
import pandas as pd

class YahooFinanceCollector(DataCollector):
    """Yahoo Finance data collector."""

    def __init__(self):
        super().__init__(api_key="", rate_limit=2000)  # No API key needed

    async def collect_stock_data(self, symbol: str, period: str = "1y") -> Dict[str, Any]:
        """Collect stock data for a given symbol."""
        try:
            ticker = yf.Ticker(symbol)

            # Collect historical data
            hist = ticker.history(period=period)

            # Collect fundamental data
            info = ticker.info

            return {
                "symbol": symbol,
                "historical_data": hist.to_dict('records'),
                "fundamentals": {
                    "market_cap": info.get("marketCap"),
                    "pe_ratio": info.get("trailingPE"),
                    "dividend_yield": info.get("dividendYield"),
                    "beta": info.get("beta"),
                    "sector": info.get("sector"),
                    "industry": info.get("industry")
                },
                "collected_at": datetime.now().isoformat()
            }
        except Exception as e:
            raise DataCollectionError(f"Failed to collect data for {symbol}: {str(e)}")

    async def collect_market_data(self, symbols: List[str]) -> List[Dict[str, Any]]:
        """Collect data for multiple symbols."""
        tasks = [self.collect_stock_data(symbol) for symbol in symbols]
        return await asyncio.gather(*tasks, return_exceptions=True)
```

#### **3. Data Transformer**
```python
from typing import Dict, Any, List
import pandas as pd
from datetime import datetime

class DataTransformer:
    """Transform raw data into standardized format."""

    @staticmethod
    def transform_stock_data(raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """Transform Yahoo Finance stock data."""
        transformed = {
            "company": {
                "symbol": raw_data["symbol"],
                "name": raw_data["fundamentals"].get("longName", ""),
                "sector": raw_data["fundamentals"].get("sector"),
                "industry": raw_data["fundamentals"].get("industry"),
                "market_cap": raw_data["fundamentals"].get("market_cap"),
                "pe_ratio": raw_data["fundamentals"].get("pe_ratio"),
                "dividend_yield": raw_data["fundamentals"].get("dividend_yield"),
                "beta": raw_data["fundamentals"].get("beta")
            },
            "prices": []
        }

        # Transform historical price data
        for record in raw_data["historical_data"]:
            transformed["prices"].append({
                "date": record["Date"].date().isoformat(),
                "open": float(record["Open"]),
                "high": float(record["High"]),
                "low": float(record["Low"]),
                "close": float(record["Close"]),
                "volume": int(record["Volume"])
            })

        return transformed

    @staticmethod
    def validate_transformed_data(data: Dict[str, Any]) -> bool:
        """Validate transformed data structure."""
        required_fields = ["company", "prices"]

        if not all(field in data for field in required_fields):
            return False

        if not data["company"].get("symbol"):
            return False

        if not data["prices"] or len(data["prices"]) == 0:
            return False

        return True
```

#### **4. Data Loader**
```python
import asyncpg
from typing import Dict, Any, List
import json

class DataLoader:
    """Load transformed data into database."""

    def __init__(self, database_url: str):
        self.database_url = database_url
        self.pool: Optional[asyncpg.Pool] = None

    async def __aenter__(self):
        self.pool = await asyncpg.create_pool(self.database_url)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.pool:
            await self.pool.close()

    async def load_company_data(self, company_data: Dict[str, Any]) -> int:
        """Load company data and return company ID."""
        async with self.pool.acquire() as conn:
            # Insert or update company
            company_id = await conn.fetchval("""
                INSERT INTO companies (symbol, name, sector, industry, market_cap, pe_ratio, dividend_yield, beta)
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                ON CONFLICT (symbol) DO UPDATE SET
                    name = EXCLUDED.name,
                    sector = EXCLUDED.sector,
                    industry = EXCLUDED.industry,
                    market_cap = EXCLUDED.market_cap,
                    pe_ratio = EXCLUDED.pe_ratio,
                    dividend_yield = EXCLUDED.dividend_yield,
                    beta = EXCLUDED.beta,
                    updated_at = CURRENT_TIMESTAMP
                RETURNING id
            """,
            company_data["symbol"],
            company_data["name"],
            company_data["sector"],
            company_data["industry"],
            company_data["market_cap"],
            company_data["pe_ratio"],
            company_data["dividend_yield"],
            company_data["beta"]
            )

            return company_id

    async def load_stock_prices(self, company_id: int, prices: List[Dict[str, Any]]):
        """Load stock price data."""
        async with self.pool.acquire() as conn:
            # Prepare batch insert
            values = [
                (company_id, price["date"], price["open"], price["high"],
                 price["low"], price["close"], price["volume"])
                for price in prices
            ]

            await conn.executemany("""
                INSERT INTO stock_prices (company_id, date, open_price, high_price, low_price, close_price, volume)
                VALUES ($1, $2, $3, $4, $5, $6, $7)
                ON CONFLICT (company_id, date) DO UPDATE SET
                    open_price = EXCLUDED.open_price,
                    high_price = EXCLUDED.high_price,
                    low_price = EXCLUDED.low_price,
                    close_price = EXCLUDED.close_price,
                    volume = EXCLUDED.volume
            """, values)
```

#### **5. Data Validator**
```python
from typing import Dict, Any, List
import pandas as pd
import numpy as np

class DataValidator:
    """Validate data quality and integrity."""

    @staticmethod
    def validate_stock_prices(prices: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Validate stock price data quality."""
        df = pd.DataFrame(prices)

        validation_results = {
            "total_records": len(prices),
            "missing_values": df.isnull().sum().to_dict(),
            "outliers": {},
            "data_quality_score": 0.0
        }

        # Check for missing values
        missing_pct = df.isnull().sum().sum() / (len(df) * len(df.columns))

        # Check for price outliers (beyond 3 standard deviations)
        if len(df) > 0:
            for col in ['open', 'high', 'low', 'close']:
                if col in df.columns:
                    mean_val = df[col].mean()
                    std_val = df[col].std()
                    outliers = df[(df[col] < mean_val - 3*std_val) |
                                (df[col] > mean_val + 3*std_val)]
                    validation_results["outliers"][col] = len(outliers)

        # Calculate data quality score (0-100)
        validation_results["data_quality_score"] = max(0, 100 - (missing_pct * 100))

        return validation_results

    @staticmethod
    def validate_financial_ratios(company_data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate financial ratios for reasonableness."""
        validation_results = {
            "pe_ratio_valid": True,
            "market_cap_valid": True,
            "dividend_yield_valid": True,
            "warnings": []
        }

        # PE ratio validation (should be positive and reasonable)
        pe_ratio = company_data.get("pe_ratio")
        if pe_ratio is not None:
            if pe_ratio <= 0 or pe_ratio > 1000:
                validation_results["pe_ratio_valid"] = False
                validation_results["warnings"].append(f"Unusual PE ratio: {pe_ratio}")

        # Market cap validation (should be positive)
        market_cap = company_data.get("market_cap")
        if market_cap is not None and market_cap <= 0:
            validation_results["market_cap_valid"] = False
            validation_results["warnings"].append(f"Invalid market cap: {market_cap}")

        # Dividend yield validation (should be 0-100%)
        dividend_yield = company_data.get("dividend_yield")
        if dividend_yield is not None:
            if dividend_yield < 0 or dividend_yield > 100:
                validation_results["dividend_yield_valid"] = False
                validation_results["warnings"].append(f"Unusual dividend yield: {dividend_yield}%")

        return validation_results
```

#### **6. Data Cache Manager**
```python
import redis
import json
from typing import Any, Optional
from datetime import timedelta

class DataCacheManager:
    """Manage Redis caching for frequently accessed data."""

    def __init__(self, redis_url: str):
        self.redis_client = redis.from_url(redis_url)
        self.default_ttl = 3600  # 1 hour

    def cache_company_data(self, symbol: str, data: Dict[str, Any], ttl: int = None):
        """Cache company data."""
        key = f"company:{symbol}"
        ttl = ttl or self.default_ttl

        self.redis_client.setex(
            key,
            ttl,
            json.dumps(data, default=str)
        )

    def get_cached_company_data(self, symbol: str) -> Optional[Dict[str, Any]]:
        """Retrieve cached company data."""
        key = f"company:{symbol}"
        data = self.redis_client.get(key)

        if data:
            return json.loads(data)
        return None

    def cache_portfolio_summary(self, portfolio_id: int, data: Dict[str, Any], ttl: int = None):
        """Cache portfolio summary data."""
        key = f"portfolio:{portfolio_id}:summary"
        ttl = ttl or self.default_ttl

        self.redis_client.setex(
            key,
            ttl,
            json.dumps(data, default=str)
        )

    def invalidate_cache(self, pattern: str):
        """Invalidate cache entries matching pattern."""
        keys = self.redis_client.keys(pattern)
        if keys:
            self.redis_client.delete(*keys)
```

#### **7. Retry Handler**
```python
import asyncio
from typing import Callable, Any
import logging

class RetryHandler:
    """Handle retries for failed operations."""

    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.logger = logging.getLogger(__name__)

    async def execute_with_retry(
        self,
        operation: Callable,
        *args,
        **kwargs
    ) -> Any:
        """Execute operation with exponential backoff retry."""
        last_exception = None

        for attempt in range(self.max_retries + 1):
            try:
                return await operation(*args, **kwargs)
            except Exception as e:
                last_exception = e

                if attempt < self.max_retries:
                    delay = self.base_delay * (2 ** attempt)
                    self.logger.warning(
                        f"Operation failed (attempt {attempt + 1}/{self.max_retries + 1}): {str(e)}. "
                        f"Retrying in {delay} seconds..."
                    )
                    await asyncio.sleep(delay)
                else:
                    self.logger.error(
                        f"Operation failed after {self.max_retries + 1} attempts: {str(e)}"
                    )
                    raise last_exception
```

#### **8. Pipeline Scheduler**
```python
import asyncio
from datetime import datetime, time
from typing import List, Dict, Any
import logging

class PipelineScheduler:
    """Schedule and orchestrate ETL pipeline execution."""

    def __init__(self):
        self.tasks: List[Dict[str, Any]] = []
        self.logger = logging.getLogger(__name__)

    def add_task(self, task_name: str, task_func: Callable, schedule: str, **kwargs):
        """Add a scheduled task to the pipeline."""
        self.tasks.append({
            "name": task_name,
            "function": task_func,
            "schedule": schedule,
            "kwargs": kwargs,
            "last_run": None,
            "next_run": self._calculate_next_run(schedule)
        })

    def _calculate_next_run(self, schedule: str) -> datetime:
        """Calculate next run time based on schedule."""
        now = datetime.now()

        if schedule == "hourly":
            return now.replace(minute=0, second=0, microsecond=0)
        elif schedule == "daily":
            return now.replace(hour=9, minute=0, second=0, microsecond=0)
        elif schedule == "weekly":
            # Run on Monday at 9 AM
            days_ahead = 7 - now.weekday()
            if days_ahead == 7:
                days_ahead = 0
            return (now + timedelta(days=days_ahead)).replace(hour=9, minute=0, second=0, microsecond=0)

        return now

    async def run_scheduled_tasks(self):
        """Run all scheduled tasks."""
        while True:
            now = datetime.now()

            for task in self.tasks:
                if task["next_run"] and now >= task["next_run"]:
                    try:
                        self.logger.info(f"Running scheduled task: {task['name']}")

                        # Execute task
                        if asyncio.iscoroutinefunction(task["function"]):
                            await task["function"](**task["kwargs"])
                        else:
                            task["function"](**task["kwargs"])

                        # Update task status
                        task["last_run"] = now
                        task["next_run"] = self._calculate_next_run(task["schedule"])

                        self.logger.info(f"Completed scheduled task: {task['name']}")

                    except Exception as e:
                        self.logger.error(f"Failed to run scheduled task {task['name']}: {str(e)}")

            # Wait for next minute
            await asyncio.sleep(60)
```

---

## üöÄ **Implementation Phases**

### **Phase 1: Core Infrastructure (Week 4)**
- [ ] Set up PostgreSQL database with initial schema
- [ ] Implement basic data collector classes
- [ ] Create data transformation framework
- [ ] Set up Redis caching layer

### **Phase 2: Data Source Integration (Week 5)**
- [ ] Implement Yahoo Finance collector
- [ ] Implement Alpha Vantage collector
- [ ] Implement FRED collector
- [ ] Add data validation and quality checks

### **Phase 3: ETL Pipeline (Week 6)**
- [ ] Implement data loading mechanisms
- [ ] Add retry logic and error handling
- [ ] Create pipeline orchestration
- [ ] Implement monitoring and alerting

### **Phase 4: Testing & Optimization (Week 7)**
- [ ] Performance testing with large datasets
- [ ] Data quality validation testing
- [ ] Error handling and recovery testing
- [ ] Performance optimization and tuning

---

## üìä **Success Metrics**

### **Performance Targets**
- **Data Collection**: 10,000+ records/hour
- **Data Processing**: <100ms per record
- **Database Queries**: <50ms for standard operations
- **Cache Hit Rate**: >90% for frequently accessed data

### **Quality Targets**
- **Data Accuracy**: >99.5%
- **Data Completeness**: >98%
- **Data Freshness**: <1 hour for real-time data
- **Error Rate**: <0.1%

### **Reliability Targets**
- **System Uptime**: >99.9%
- **Data Pipeline Success Rate**: >99.5%
- **Recovery Time**: <5 minutes for critical failures
- **Data Loss**: 0% (with backup and recovery)

---

## üîß **Configuration & Environment**

### **Environment Variables**
```bash
# Database Configuration
DATABASE_URL=postgresql://user:password@localhost:5432/investbyyourself
REDIS_URL=redis://localhost:6379/0

# API Keys
YAHOO_FINANCE_API_KEY=
ALPHA_VANTAGE_API_KEY=
FRED_API_KEY=
API_NINJAS_API_KEY=

# ETL Configuration
ETL_BATCH_SIZE=1000
ETL_MAX_WORKERS=4
ETL_RETRY_ATTEMPTS=3
ETL_RETRY_DELAY=5

# Monitoring
LOG_LEVEL=INFO
METRICS_ENABLED=true
ALERT_EMAIL=alerts@investbyyourself.com
```

### **Docker Compose Configuration**
```yaml
version: '3.8'
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: investbyyourself
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  etl-worker:
    build: .
    environment:
      - DATABASE_URL=postgresql://user:password@postgres:5432/investbyyourself
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - postgres
      - redis

volumes:
  postgres_data:
  redis_data:
```

---

## üìù **Next Steps**

1. **Database Setup**: Install and configure PostgreSQL
2. **Schema Creation**: Create initial database schema
3. **Core Classes**: Implement base data collector and transformer classes
4. **First Integration**: Start with Yahoo Finance integration
5. **Testing**: Create comprehensive tests for ETL pipeline
6. **Monitoring**: Implement data quality monitoring and alerting

---

**Last Updated**: January 2025
**Maintained By**: investByYourself Development Team
**Status**: üöß IN PROGRESS
